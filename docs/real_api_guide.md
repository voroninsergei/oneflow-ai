# OneFlow.AI - Real API Integration Guide
## –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ API

---

## üéØ –ß—Ç–æ –Ω–æ–≤–æ–≥–æ –≤ –≠—Ç–∞–ø–µ 3

### ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ:

1. **Enhanced Real API Module** (`enhanced_real_api.py`)
   - ‚úÖ Retry logic —Å exponential backoff
   - ‚úÖ Rate limiting –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
   - ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
   - ‚úÖ Mock —Ä–µ–∂–∏–º –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
   - ‚úÖ –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

2. **Provider Manager** (`provider_manager.py`)
   - ‚úÖ –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏
   - ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ fallbacks (GPT ‚Üí Claude, Stability ‚Üí DALL-E)
   - ‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
   - ‚úÖ Health checks –∏ —Å—Ç–∞—Ç—É—Å—ã
   - ‚úÖ –í—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞

3. **API Keys Management** (—É–∂–µ –±—ã–ª)
   - ‚úÖ –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ `.api_keys.json`
   - ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ environment variables
   - ‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–ª—é—á–µ–π

---

## üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
# –û—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (—É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã)
pip install -r requirements.txt

# –î–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö API –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
pip install openai anthropic requests

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –¥–ª—è async –æ–ø–µ—Ä–∞—Ü–∏–π
pip install aiohttp asyncio
```

---

## üîë –ù–∞—Å—Ç—Ä–æ–π–∫–∞ API –∫–ª—é—á–µ–π

### –°–ø–æ—Å–æ–± 1: –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞

```bash
python setup_keys.py
```

–°–ª–µ–¥—É–π—Ç–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–ª—é—á–µ–π.

### –°–ø–æ—Å–æ–± 2: –†—É—á–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ `.api_keys.json`

```json
{
  "openai": "sk-proj-your-openai-key-here",
  "anthropic": "sk-ant-your-anthropic-key-here",
  "stability": "sk-your-stability-key-here",
  "elevenlabs": "your-elevenlabs-key-here"
}
```

**–í–ê–ñ–ù–û**: –î–æ–±–∞–≤—å—Ç–µ –≤ `.gitignore`:
```bash
echo ".api_keys.json" >> .gitignore
chmod 600 .api_keys.json
```

### –°–ø–æ—Å–æ–± 3: Environment Variables

```bash
# Linux/Mac
export OPENAI_API_KEY="sk-proj-..."
export ANTHROPIC_API_KEY="sk-ant-..."
export STABILITY_API_KEY="sk-..."
export ELEVENLABS_API_KEY="..."

# Windows PowerShell
$env:OPENAI_API_KEY="sk-proj-..."
$env:ANTHROPIC_API_KEY="sk-ant-..."
```

---

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### –ü—Ä–∏–º–µ—Ä 1: –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Enhanced API

```python
from enhanced_real_api import create_enhanced_provider

# –°–æ–∑–¥–∞—Ç—å OpenAI –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
gpt_provider = create_enhanced_provider('gpt')

# –¢–µ–∫—Å—Ç–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
result = gpt_provider(
    'Write a haiku about AI',
    task_type='text',
    model='gpt-3.5-turbo'
)

print(f"Response: {result['response']}")
print(f"Cost: ${result['cost']:.4f}")
print(f"Tokens: {result['tokens_used']}")

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
result = gpt_provider(
    'A beautiful sunset',
    task_type='image',
    size='1024x1024'
)

print(f"Image URL: {result['image_url']}")
print(f"Cost: ${result['cost']:.4f}")
```

### –ü—Ä–∏–º–µ—Ä 2: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Provider Manager

```python
from provider_manager import initialize_provider_manager

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ API
manager = initialize_provider_manager(use_real_api=True)

# –ó–∞–ø—Ä–æ—Å —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback
# –ü–æ–ø—Ä–æ–±—É–µ—Ç OpenAI, –µ—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏—Ç—Å—è - Anthropic
result = manager.execute_with_fallback(
    'text',
    'Explain quantum computing simply'
)

print(f"Provider used: {result['provider_name']}")
print(f"Response: {result['response']}")
print(f"Response time: {result['response_time']:.2f}s")

# –ü—Ä–æ—Å–º–æ—Ç—Ä –º–µ—Ç—Ä–∏–∫
summary = manager.get_metrics_summary()
print(f"Total requests: {summary['total_requests']}")
print(f"Success rate: {summary['overall_success_rate']}")
```

### –ü—Ä–∏–º–µ—Ä 3: –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã —Å fallback

```python
from provider_manager import initialize_provider_manager

manager = initialize_provider_manager(use_real_api=True)

requests = [
    ('text', 'Write a poem about AI'),
    ('text', 'Explain machine learning'),
    ('image', 'A futuristic city'),
    ('audio', 'Welcome to OneFlow.AI'),
]

for task_type, prompt in requests:
    result = manager.execute_with_fallback(task_type, prompt)
    
    if result['status'] == 'success':
        print(f"‚úì {task_type}: {result['provider_name']} - ${result.get('cost', 0):.4f}")
    else:
        print(f"‚úó {task_type}: {result['error']}")

# –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
status = manager.get_all_providers_status()
for name, info in status.items():
    metrics = info['metrics']
    print(f"\n{name}:")
    print(f"  Success rate: {metrics['success_rate']}")
    print(f"  Total cost: {metrics['total_cost']}")
```

---

## üîß –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º

### –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ `src/main.py`

```python
"""
OneFlow.AI Main Module - Enhanced with Real API
"""

from provider_manager import initialize_provider_manager
from pricing import PricingCalculator
from wallet import Wallet
from analytics import Analytics
from budget import Budget, BudgetPeriod


class OneFlowAI:
    """Enhanced OneFlow.AI with real API support."""
    
    def __init__(self, initial_balance: float = 100, use_real_api: bool = False):
        self.wallet = Wallet(initial_balance=initial_balance)
        self.pricing = PricingCalculator()
        self.analytics = Analytics()
        self.budget = Budget()
        self.use_real_api = use_real_api
        
        # Initialize Provider Manager
        self.provider_manager = initialize_provider_manager(use_real_api)
        
        # Setup pricing from provider manager
        self._setup_pricing()
    
    def _setup_pricing(self):
        """Setup pricing rates."""
        rates = {
            'gpt': 1.0,
            'image': 10.0,
            'audio': 5.0,
            'video': 20.0
        }
        for provider, rate in rates.items():
            self.pricing.register_rate(provider, rate)
    
    def process_request(self, model: str, prompt: str, **kwargs) -> dict:
        """Process request with provider manager."""
        model_lower = model.lower()
        
        # Map model to task type
        task_type_map = {
            'gpt': 'text',
            'image': 'image',
            'audio': 'audio',
            'video': 'video'
        }
        task_type = task_type_map.get(model_lower, 'text')
        
        # Calculate cost
        if model_lower == 'gpt':
            cost_units = len(prompt.split())
        else:
            cost_units = 1
        
        cost = self.pricing.estimate_cost(model_lower, cost_units)
        
        # Check budget
        can_spend, reason = self.budget.can_spend(cost, provider=model_lower)
        if not can_spend:
            return {
                'status': 'error',
                'message': f'Budget limit: {reason}',
                'balance': self.wallet.get_balance()
            }
        
        # Check wallet
        if not self.wallet.can_afford(cost):
            return {
                'status': 'error',
                'message': f'Insufficient funds',
                'balance': self.wallet.get_balance()
            }
        
        # Execute with provider manager (automatic fallback)
        result = self.provider_manager.execute_with_fallback(
            task_type, prompt, **kwargs
        )
        
        if result['status'] == 'success':
            # Deduct cost
            self.wallet.deduct(cost)
            self.budget.record_spending(cost, provider=model_lower)
            
            # Log analytics
            self.analytics.log_request(
                result['provider_name'],
                cost,
                prompt,
                status='success',
                response=str(result.get('response', ''))
            )
            
            return {
                'status': 'success',
                'response': result,
                'cost': cost,
                'balance': self.wallet.get_balance(),
                'provider': result['provider_name'],
                'response_time': result.get('response_time', 0)
            }
        else:
            # Log failure
            self.analytics.log_request(
                'unknown',
                0,
                prompt,
                status='error',
                response=result.get('error', '')
            )
            
            return {
                'status': 'error',
                'message': result.get('error', 'Unknown error'),
                'balance': self.wallet.get_balance()
            }
    
    def get_provider_status(self):
        """Get all providers status."""
        return self.provider_manager.get_all_providers_status()
    
    def get_best_provider(self, task_type: str):
        """Get best provider for task type."""
        return self.provider_manager.get_best_provider(task_type)


# Example usage
if __name__ == '__main__':
    # Initialize with real API
    system = OneFlowAI(initial_balance=100, use_real_api=True)
    
    # Make request
    result = system.process_request('gpt', 'Hello, world!')
    
    if result['status'] == 'success':
        print(f"‚úì Success!")
        print(f"  Provider: {result['provider']}")
        print(f"  Cost: {result['cost']:.2f}")
        print(f"  Response time: {result['response_time']:.2f}s")
    else:
        print(f"‚úó Error: {result['message']}")
    
    # Check provider status
    status = system.get_provider_status()
    for name, info in status.items():
        print(f"\n{name}: {info['status']}")
        print(f"  Success rate: {info['metrics']['success_rate']}")
```

---

## üéØ –ö–ª—é—á–µ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

### 1. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ Fallbacks

```python
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ fallback —Ü–µ–ø–æ—á–µ–∫
manager.set_fallback_chain('text', ['openai', 'anthropic', 'gpt'])
manager.set_fallback_chain('image', ['stability', 'openai'])

# –ó–∞–ø—Ä–æ—Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–æ–±—É–µ—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –ø–æ –ø–æ—Ä—è–¥–∫—É
result = manager.execute_with_fallback('text', 'Hello')
# –ü–æ–ø—Ä–æ–±—É–µ—Ç: OpenAI ‚Üí –µ—Å–ª–∏ –æ—à–∏–±–∫–∞ ‚Üí Anthropic ‚Üí –µ—Å–ª–∏ –æ—à–∏–±–∫–∞ ‚Üí GPT mock
```

### 2. Rate Limiting

```python
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π rate limiting (–≤—Å—Ç—Ä–æ–µ–Ω –≤ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã)
# OpenAI: 60 requests/minute
# Anthropic: 50 requests/minute
# Stability: 150 requests/minute
# ElevenLabs: 120 requests/minute

# –ï—Å–ª–∏ –ª–∏–º–∏—Ç –¥–æ—Å—Ç–∏–≥–Ω—É—Ç - –ø—Ä–æ–≤–∞–π–¥–µ—Ä –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∂–¥—ë—Ç
for i in range(100):
    result = gpt_provider('Request ' + str(i))
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–ø—Ä–∞–≤–ª—è–µ—Ç rate limits
```

### 3. Retry Logic —Å Exponential Backoff

```python
# –í—Å—Ç—Ä–æ–µ–Ω –≤ –∫–∞–∂–¥—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä —á–µ—Ä–µ–∑ –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä @retry_with_backoff
# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: max_retries=3, base_delay=1.0

# –ü—Ä–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –æ—à–∏–±–∫–µ:
# - –ü–æ–ø—ã—Ç–∫–∞ 1: –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ
# - –ü–æ–ø—ã—Ç–∫–∞ 2: —á–µ—Ä–µ–∑ 1 —Å–µ–∫—É–Ω–¥—É
# - –ü–æ–ø—ã—Ç–∫–∞ 3: —á–µ—Ä–µ–∑ 2 —Å–µ–∫—É–Ω–¥—ã
# - –ü–æ–ø—ã—Ç–∫–∞ 4: —á–µ—Ä–µ–∑ 4 —Å–µ–∫—É–Ω–¥—ã
# - –ï—Å–ª–∏ –≤—Å–µ –Ω–µ—É–¥–∞—á–Ω—ã - –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏–µ
```

### 4. –ú–µ—Ç—Ä–∏–∫–∏ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

```python
# –ü–æ–ª—É—á–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
status = manager.get_provider_status('openai')
print(f"Total requests: {status['metrics']['total_requests']}")
print(f"Success rate: {status['metrics']['success_rate']}")
print(f"Average response time: {status['metrics']['average_response_time']}")
print(f"Total cost: {status['metrics']['total_cost']}")

# –°–≤–æ–¥–∫–∞ –ø–æ –≤—Å–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º
summary = manager.get_metrics_summary()
print(summary)

# –°–±—Ä–æ—Å –º–µ—Ç—Ä–∏–∫
manager.reset_metrics('openai')  # –î–ª—è –æ–¥–Ω–æ–≥–æ
manager.reset_metrics()  # –î–ª—è –≤—Å–µ—Ö
```

### 5. Health Checks

```python
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
if manager.providers['openai']['status'] == ProviderStatus.AVAILABLE:
    print("OpenAI –¥–æ—Å—Ç—É–ø–µ–Ω")
else:
    print("OpenAI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

# –í—Ä—É—á–Ω—É—é –∏–∑–º–µ–Ω–∏—Ç—å —Å—Ç–∞—Ç—É—Å
manager.mark_provider_unavailable('openai', 'Maintenance')
manager.mark_provider_available('openai')
```

---

## üìä –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –°—Ü–µ–Ω–∞—Ä–∏–π 1: –ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

```python
from provider_manager import initialize_provider_manager

# Production setup
manager = initialize_provider_manager(use_real_api=True)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ fallbacks
manager.set_fallback_chain('text', ['anthropic', 'openai'])  # Claude first

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
user_prompts = [
    "Translate 'hello' to Spanish",
    "What is 2+2?",
    "Write a poem about stars"
]

for prompt in user_prompts:
    try:
        result = manager.execute_with_fallback('text', prompt)
        
        if result['status'] == 'success':
            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            save_to_database(
                user_id=current_user.id,
                prompt=prompt,
                response=result['response'],
                provider=result['provider_name'],
                cost=result['cost']
            )
        else:
            # –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å –æ—à–∏–±–∫—É
            log_error(f"Failed to process: {result['error']}")
    
    except Exception as e:
        # Critical error handling
        alert_admin(f"Critical API failure: {e}")
```

### –°—Ü–µ–Ω–∞—Ä–∏–π 2: A/B Testing –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤

```python
import random

def compare_providers(prompt: str):
    """Compare GPT vs Claude for same prompt."""
    
    # Test with OpenAI
    gpt_provider = create_enhanced_provider('gpt')
    gpt_result = gpt_provider(prompt, task_type='text')
    
    # Test with Anthropic
    claude_provider = create_enhanced_provider('claude')
    claude_result = claude_provider(prompt)
    
    # Compare results
    comparison = {
        'prompt': prompt,
        'gpt': {
            'response': gpt_result['response'],
            'cost': gpt_result['cost'],
            'tokens': gpt_result['tokens_used'],
            'response_time': gpt_result.get('response_time', 0)
        },
        'claude': {
            'response': claude_result['response'],
            'cost': claude_result['cost'],
            'tokens': claude_result['tokens_used'],
            'response_time': claude_result.get('response_time', 0)
        }
    }
    
    return comparison

# Run A/B test
test_prompts = [
    "Explain quantum physics simply",
    "Write a creative story",
    "Solve this math problem: 234 * 567"
]

for prompt in test_prompts:
    result = compare_providers(prompt)
    print(f"\nPrompt: {prompt}")
    print(f"GPT cost: ${result['gpt']['cost']:.4f}")
    print(f"Claude cost: ${result['claude']['cost']:.4f}")
    print(f"Winner: {'GPT' if result['gpt']['cost'] < result['claude']['cost'] else 'Claude'}")
```

### –°—Ü–µ–Ω–∞—Ä–∏–π 3: Batch Processing —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º

```python
from provider_manager import initialize_provider_manager
from tqdm import tqdm

def process_batch(prompts: list, task_type: str = 'text'):
    """Process batch of prompts with progress tracking."""
    manager = initialize_provider_manager(use_real_api=True)
    results = []
    
    for prompt in tqdm(prompts, desc="Processing"):
        result = manager.execute_with_fallback(task_type, prompt)
        results.append(result)
        
        # Optional: delay between requests
        time.sleep(0.5)
    
    return results

# Usage
prompts = [f"Generate idea #{i}" for i in range(100)]
results = process_batch(prompts)

# Analyze results
successful = sum(1 for r in results if r['status'] == 'success')
total_cost = sum(r.get('cost', 0) for r in results)

print(f"Success rate: {successful/len(results)*100:.1f}%")
print(f"Total cost: ${total_cost:.2f}")
```

---

## üîç Troubleshooting

### –ü—Ä–æ–±–ª–µ–º–∞ 1: "API key not configured"

**–†–µ—à–µ–Ω–∏–µ:**
```bash
# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ .api_keys.json
cat .api_keys.json

# –ò–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
echo $OPENAI_API_KEY

# –ó–∞–ø—É—Å—Ç–∏—Ç–µ setup
python setup_keys.py
```

### –ü—Ä–æ–±–ª–µ–º–∞ 2: "Rate limit exceeded"

**–†–µ—à–µ–Ω–∏–µ:**
```python
# Rate limiter —É–∂–µ –≤—Å—Ç—Ä–æ–µ–Ω, –Ω–æ –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å:
from enhanced_real_api import _rate_limiters

# –£–≤–µ–ª–∏—á–∏—Ç—å –ª–∏–º–∏—Ç—ã (–µ—Å–ª–∏ —É –≤–∞—Å Premium –∞–∫–∫–∞—É–Ω—Ç)
_rate_limiters['openai'] = RateLimiter(max_requests=100, time_window=60)
```

### –ü—Ä–æ–±–ª–µ–º–∞ 3: "All providers failed"

**–†–µ—à–µ–Ω–∏–µ:**
```python
# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç–∞—Ç—É—Å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
status = manager.get_all_providers_status()
for name, info in status.items():
    print(f"{name}: {info['status']}")
    if info['metrics']['last_error']:
        print(f"  Last error: {info['metrics']['last_error']}")

# –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≤—Ä—É—á–Ω—É—é
provider = manager.get_provider('openai')
result = provider('Test prompt', task_type='text')
print(result)
```

### –ü—Ä–æ–±–ª–µ–º–∞ 4: "Import Error"

**–†–µ—à–µ–Ω–∏–µ:**
```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø–∞–∫–µ—Ç—ã
pip install openai anthropic requests

# –ò–ª–∏ –¥–ª—è –≤—Å–µ—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
pip install -r requirements.txt
pip install openai anthropic requests
```

---

## üìà –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è

```python
import logging

# –ù–∞—Å—Ç—Ä–æ–∏—Ç—å —É—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('oneflow.log'),
        logging.StreamHandler()
    ]
)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ –∫–æ–¥–µ
logger = logging.getLogger('OneFlow.AI')

# –õ–æ–≥–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –±—É–¥—É—Ç –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å:
# - –ö–∞–∫–æ–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
# - –û—à–∏–±–∫–∏ –∏ retry –ø–æ–ø—ã—Ç–∫–∏
# - Rate limiting —Å–æ–±—ã—Ç–∏—è
# - –£—Å–ø–µ—à–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
```

### –≠–∫—Å–ø–æ—Ä—Ç –º–µ—Ç—Ä–∏–∫

```python
import json
from datetime import datetime

def export_metrics(manager, filename=None):
    """Export provider metrics to JSON."""
    if filename is None:
        filename = f"metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    metrics = manager.get_metrics_summary()
    
    with open(filename, 'w') as f:
        json.dump(metrics, f, indent=2)
    
    print(f"‚úì Metrics exported to {filename}")
    return filename

# Usage
manager = initialize_provider_manager(use_real_api=True)
# ... make some requests ...
export_metrics(manager)
```

---

## üîí –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å

### Best Practices:

1. **–ù–∏–∫–æ–≥–¥–∞ –Ω–µ –∫–æ–º–º–∏—Ç—å—Ç–µ API –∫–ª—é—á–∏**
   ```bash
   # .gitignore
   .api_keys.json
   *.key
   *.pem
   .env
   ```

2. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –≤ production**
   ```bash
   # –í production
   export OPENAI_API_KEY="sk-..."
   # –ù–ï —Ö—Ä–∞–Ω–∏—Ç–µ –≤ –∫–æ–¥–µ!
   ```

3. **–†–æ—Ç–∞—Ü–∏—è –∫–ª—é—á–µ–π**
   ```python
   # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–ª—è–π—Ç–µ –∫–ª—é—á–∏
   # 1. –°–æ–∑–¥–∞–π—Ç–µ –Ω–æ–≤—ã–π –∫–ª—é—á –Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ
   # 2. –û–±–Ω–æ–≤–∏—Ç–µ .api_keys.json
   # 3. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
   # 4. –£–¥–∞–ª–∏—Ç–µ —Å—Ç–∞—Ä—ã–π –∫–ª—é—á —á–µ—Ä–µ–∑ 24 —á–∞—Å–∞
   ```

4. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è**
   ```python
   # –†–µ–≥—É–ª—è—Ä–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ –∑–∞—Ç—Ä–∞—Ç—ã
   summary = manager.get_metrics_summary()
   if float(summary['total_cost'].replace(', '')) > 100:
       alert_admin("High API costs detected!")
   ```

---

## üí∞ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞—Ç—Ä–∞—Ç

### –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –í—ã–±–æ—Ä –¥–µ—à—ë–≤—ã—Ö –º–æ–¥–µ–ª–µ–π

```python
# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ gpt-3.5-turbo –≤–º–µ—Å—Ç–æ gpt-4
result = gpt_provider(
    'Simple question',
    task_type='text',
    model='gpt-3.5-turbo'  # $0.002/1K vs $0.03/1K
)

# Claude Haiku –≤–º–µ—Å—Ç–æ Opus
result = claude_provider(
    'Simple question',
    model='claude-3-haiku-20240307'  # –î–µ—à–µ–≤–ª–µ
)
```

### –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ

```python
import hashlib
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_request(prompt_hash: str, task_type: str):
    """Cache API responses."""
    # –†–µ–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –≤ –∫–µ—à–µ
    return manager.execute_with_fallback(task_type, prompt)

# Usage
prompt = "What is AI?"
prompt_hash = hashlib.md5(prompt.encode()).hexdigest()
result = cached_request(prompt_hash, 'text')
```

### –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: Batch Processing

```python
# –ì—Ä—É–ø–ø–∏—Ä—É–π—Ç–µ –∑–∞–ø—Ä–æ—Å—ã
prompts = ["Question 1", "Question 2", "Question 3"]

# –í–º–µ—Å—Ç–æ 3 –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤:
combined_prompt = "\n".join([f"{i+1}. {p}" for i, p in enumerate(prompts)])
result = gpt_provider(
    f"Answer these questions:\n{combined_prompt}",
    task_type='text'
)
# –û–¥–Ω–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è –≤–º–µ—Å—Ç–æ —Ç—Ä—ë—Ö
```

---

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### Unit —Ç–µ—Å—Ç—ã –¥–ª—è Real API

```python
import pytest
from enhanced_real_api import create_enhanced_provider

def test_openai_provider_text():
    """Test OpenAI text generation."""
    provider = create_enhanced_provider('gpt')
    result = provider('Test prompt', task_type='text', model='gpt-3.5-turbo')
    
    assert result['status'] == 'success' or 'error' in result
    assert 'cost' in result
    assert 'response' in result or 'error' in result

def test_provider_manager_fallback():
    """Test automatic fallback."""
    from provider_manager import initialize_provider_manager
    
    manager = initialize_provider_manager(use_real_api=True)
    
    # Mark first provider as unavailable
    manager.mark_provider_unavailable('openai', 'Testing')
    
    # Should fallback to anthropic
    result = manager.execute_with_fallback('text', 'Test')
    
    if result['status'] == 'success':
        assert result['provider_name'] == 'anthropic'

def test_rate_limiting():
    """Test rate limiter."""
    from enhanced_real_api import RateLimiter
    
    limiter = RateLimiter(max_requests=2, time_window=1)
    
    assert limiter.can_proceed() == True
    limiter.add_request()
    assert limiter.can_proceed() == True
    limiter.add_request()
    assert limiter.can_proceed() == False  # Limit reached

# Run tests
pytest.main([__file__, '-v'])
```

---

## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤

### –°—Ç–æ–∏–º–æ—Å—Ç—å (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ):

| –ü—Ä–æ–≤–∞–π–¥–µ—Ä | –¢–∏–ø | –°—Ç–æ–∏–º–æ—Å—Ç—å | –ü—Ä–∏–º–µ—á–∞–Ω–∏—è |
|-----------|-----|-----------|------------|
| OpenAI GPT-3.5 | Text | $0.002/1K tokens | –ë—ã—Å—Ç—Ä—ã–π, –¥–µ—à—ë–≤—ã–π |
| OpenAI GPT-4 | Text | $0.03/1K tokens | –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π, –¥–æ—Ä–æ–≥–æ–π |
| Anthropic Claude Haiku | Text | $0.00025/1K tokens | –û—á–µ–Ω—å –¥–µ—à—ë–≤—ã–π |
| Anthropic Claude Sonnet | Text | $0.003/1K tokens | –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π |
| Anthropic Claude Opus | Text | $0.015/1K tokens | –ü—Ä–µ–º–∏—É–º –∫–∞—á–µ—Å—Ç–≤–æ |
| OpenAI DALL-E 3 | Image | $0.04-0.12/image | HD quality |
| Stability AI SDXL | Image | $0.01-0.03/image | –ë–æ–ª–µ–µ –¥–µ—à—ë–≤—ã–π |
| ElevenLabs | Audio | $0.30/1K chars | –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π TTS |

### –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:

| –ü—Ä–æ–≤–∞–π–¥–µ—Ä | –°–∫–æ—Ä–æ—Å—Ç—å | –ö–∞—á–µ—Å—Ç–≤–æ | –ù–∞–¥—ë–∂–Ω–æ—Å—Ç—å |
|-----------|----------|----------|------------|
| OpenAI GPT-3.5 | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| OpenAI GPT-4 | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Anthropic Claude | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Stability AI | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| ElevenLabs | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |

---

## üéì –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

–ü–æ—Å–ª–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Real API –≤—ã –º–æ–∂–µ—Ç–µ:

1. ‚úÖ **–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å Web Server**
   ```python
   # –í web_server.py
   from provider_manager import initialize_provider_manager
   
   manager = initialize_provider_manager(use_real_api=True)
   ```

2. ‚úÖ **–î–æ–±–∞–≤–∏—Ç—å –≤ Database Module**
   ```python
   # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–µ
   db.create_request(
       provider=result['provider_name'],
       response_time=result['response_time'],
       # ...
   )
   ```

3. ‚úÖ **–°–æ–∑–¥–∞—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ dashboard**
   - –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
   - –ì—Ä–∞—Ñ–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
   - Alerts –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ –±—é–¥–∂–µ—Ç–∞

4. ‚úÖ **–ù–∞—Å—Ç—Ä–æ–∏—Ç—å CI/CD**
   - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
   - –î–µ–ø–ª–æ–π —Å –Ω–æ–≤—ã–º–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏

---

## ‚úÖ –ß–µ–∫–ª–∏—Å—Ç –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏

- [ ] –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: `pip install openai anthropic requests`
- [ ] –ù–∞—Å—Ç—Ä–æ–µ–Ω—ã API –∫–ª—é—á–∏ –≤ `.api_keys.json` –∏–ª–∏ env vars
- [ ] `.api_keys.json` –¥–æ–±–∞–≤–ª–µ–Ω –≤ `.gitignore`
- [ ] –°–æ—Ö—Ä–∞–Ω—ë–Ω `enhanced_real_api.py`
- [ ] –°–æ—Ö—Ä–∞–Ω—ë–Ω `provider_manager.py`
- [ ] –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω –±–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
- [ ] –ü—Ä–æ–≤–µ—Ä–µ–Ω–∞ —Ä–∞–±–æ—Ç–∞ fallbacks
- [ ] –ù–∞—Å—Ç—Ä–æ–µ–Ω–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
- [ ] –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∑–∞—Ç—Ä–∞—Ç

---

## üéâ –ì–æ—Ç–æ–≤–æ!

–¢–µ–ø–µ—Ä—å OneFlow.AI –∏–º–µ–µ—Ç:
- ‚úÖ –ü–æ–ª–Ω—É—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ AI API
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ fallbacks –º–µ–∂–¥—É –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏
- ‚úÖ Rate limiting –∏ retry –ª–æ–≥–∏–∫—É
- ‚úÖ –î–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
- ‚úÖ Production-ready –∫–æ–¥

**–≠—Ç–∞–ø 3 –∑–∞–≤–µ—Ä—à—ë–Ω!** üöÄ

**–°–ª–µ–¥—É—é—â–∏–π —ç—Ç–∞–ø: Authentication & Security** üîê

---

## üìû –ü–æ–¥–¥–µ—Ä–∂–∫–∞

–ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã:
1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏: `tail -f oneflow.log`
2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç–∞—Ç—É—Å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤: `manager.get_all_providers_status()`
3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ API –∫–ª—é—á–∏: `python -c "from enhanced_real_api import _key_manager; print(_key_manager.keys)"`
4. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞

---

**–ê–≤—Ç–æ—Ä**: Sergey Voronin  
**–í–µ—Ä—Å–∏—è**: 2.0 - Stage 3  
**–î–∞—Ç–∞**: 2025